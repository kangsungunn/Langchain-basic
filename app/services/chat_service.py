"""
ì±„íŒ… ì„œë¹„ìŠ¤

ì±„íŒ… ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

ğŸ˜ğŸ˜ chat_service.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

ë‹¨ìˆœ ì±„íŒ…/ëŒ€í™”í˜• LLM ì¸í„°í˜ì´ìŠ¤.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.

QLoRA ê¸°ë°˜ íŒŒì¸íŠœë‹ ë° ëŒ€í™” ì§€ì›.

"""

import os
from datetime import datetime
from typing import Any, Dict, List, Optional

from app.services.rag_service import RAGService


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self, rag_service: RAGService):
        """
        ì±„íŒ… ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            rag_service: RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        """
        self.rag_service = rag_service

    def chat_rag(self, message: str) -> dict:
        """
        RAG ëª¨ë“œë¡œ ì±„íŒ…í•©ë‹ˆë‹¤.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.rag_service.search_relevant_documents(message, k=3)

        if not relevant_docs:
            # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
            answer = self.rag_service.generate_answer(message, context=None)
            sources = ["ğŸ’¬ ì¶œì²˜: LLM (ì§€ì‹ ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)"]
        else:
            # RAG ëª¨ë“œ
            docs = [doc for doc, score in relevant_docs]
            context = "\n\n---\n\n".join([doc.page_content for doc in docs])
            answer = self.rag_service.generate_answer(message, context=context)

            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = [f"ğŸ“š ì¶œì²˜: {self.rag_service.llm.get_model_name()} + Vector DB"]
            for doc, score in relevant_docs:
                preview = doc.page_content[:80].replace("\n", " ").strip()
                if len(doc.page_content) > 80:
                    preview += "..."
                sources.append(f"{preview} (ìœ ì‚¬ë„: {1 - score:.2f})")

        return {
            "answer": answer,
            "sources": sources,
            "timestamp": datetime.now().isoformat(),
        }

    def chat_general(self, message: str) -> dict:
        """
        ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì±„íŒ…í•©ë‹ˆë‹¤.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        answer = self.rag_service.generate_answer(message, context=None)
        model_name = self.rag_service.llm.get_model_name()

        return {
            "answer": answer,
            "sources": [f"ğŸ’¬ ì¶œì²˜: {model_name} (ì¼ë°˜ ëŒ€í™” ëª¨ë“œ)"],
            "timestamp": datetime.now().isoformat(),
        }

    def load_qlora_model(
        self,
        model_name: str = "beomi/Llama-3-Open-Ko-8B",
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.05,
        target_modules: Optional[List[str]] = None,
    ) -> tuple:
        """
        QLoRA ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            model_name: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„
            lora_r: LoRA rank
            lora_alpha: LoRA alpha
            lora_dropout: LoRA dropout
            target_modules: LoRAë¥¼ ì ìš©í•  íƒ€ê²Ÿ ëª¨ë“ˆ

        Returns:
            (model, tokenizer) íŠœí”Œ
        """
        # Lazy import for QLoRA dependencies
        try:
            import torch
            from peft import (
                LoraConfig,
                TaskType,
                get_peft_model,
                prepare_model_for_kbit_training,
            )
            from transformers import (
                AutoModelForCausalLM,
                AutoTokenizer,
                BitsAndBytesConfig,
            )
        except ImportError as e:
            raise ImportError(
                f"QLoRA ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: "
                f"pip install torch transformers peft bitsandbytes accelerate\n"
                f"Error: {e}"
            )

        # BitsAndBytes ì„¤ì • (4bit ì–‘ìí™”)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

        # í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
        model = prepare_model_for_kbit_training(model)

        # LoRA ì„¤ì •
        if target_modules is None:
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        # PEFT ëª¨ë¸ ìƒì„±
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

        return model, tokenizer

    def chat_with_qlora_model(
        self,
        model,
        tokenizer,
        message: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> dict:
        """
        QLoRA ëª¨ë¸ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.

        Args:
            model: QLoRA ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„ (ë‹¤ì–‘ì„± ì¡°ì ˆ)
            top_p: Top-p ìƒ˜í”Œë§

        Returns:
            ë‹µë³€ê³¼ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        import torch

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>

{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        answer = generated_text.split("assistant")[-1].strip()

        return {
            "answer": answer,
            "sources": ["ğŸ¤– ì¶œì²˜: QLoRA Fine-tuned Model"],
            "timestamp": datetime.now().isoformat(),
            "model_info": {
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_new_tokens,
            },
        }

    def prepare_training_dataset(
        self, tokenizer, conversations: List[Dict[str, str]], max_length: int = 512
    ):
        """
        í•™ìŠµ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

        Args:
            tokenizer: í† í¬ë‚˜ì´ì €
            conversations: [{"prompt": "ì§ˆë¬¸", "response": "ë‹µë³€"}, ...] í˜•ì‹ì˜ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            ì¤€ë¹„ëœ Dataset ê°ì²´
        """
        try:
            from datasets import Dataset  # type: ignore
        except ImportError:
            raise ImportError("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install datasets")

        def format_prompt(prompt: str, response: str) -> str:
            """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
            return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{response}<|eot_id|>"""

        # ë°ì´í„° í¬ë§·íŒ…
        formatted_data = []
        for conv in conversations:
            text = format_prompt(conv["prompt"], conv["response"])
            formatted_data.append({"text": text})

        # Dataset ìƒì„±
        dataset = Dataset.from_list(formatted_data)

        # í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_length,
                padding="max_length",
            )

        # í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_dataset = dataset.map(
            tokenize_function, batched=True, remove_columns=dataset.column_names
        )

        return tokenized_dataset

    def train_qlora_model(
        self,
        model,
        tokenizer,
        train_dataset,
        output_dir: str = "./checkpoints/qlora",
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 100,
    ) -> Dict[str, Any]:
        """
        QLoRA ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

        Args:
            model: QLoRA ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
            output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
            num_train_epochs: ì—í­ ìˆ˜
            per_device_train_batch_size: ë°°ì¹˜ í¬ê¸°
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
            learning_rate: í•™ìŠµë¥ 
            warmup_steps: ì›Œë°ì—… ìŠ¤í…
            logging_steps: ë¡œê¹… ì£¼ê¸°
            save_steps: ì €ì¥ ì£¼ê¸°

        Returns:
            í•™ìŠµ ê²°ê³¼ ì •ë³´
        """
        try:
            from transformers import Trainer, TrainingArguments
        except ImportError:
            raise ImportError(
                "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install transformers"
            )

        # í•™ìŠµ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=3,
            fp16=True,
            optim="paged_adamw_8bit",
            report_to="none",
        )

        # Trainer ìƒì„±
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,  # type: ignore
        )

        # í•™ìŠµ ì‹œì‘
        print("ğŸš€ QLoRA í•™ìŠµ ì‹œì‘...")
        train_result = trainer.train()

        # ëª¨ë¸ ì €ì¥
        final_model_path = os.path.join(output_dir, "final_model")
        trainer.save_model(final_model_path)
        tokenizer.save_pretrained(final_model_path)

        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {final_model_path}")

        return {
            "status": "completed",
            "output_dir": output_dir,
            "final_model_path": final_model_path,
            "train_loss": train_result.training_loss,
            "epochs": num_train_epochs,
            "timestamp": datetime.now().isoformat(),
        }

    def load_trained_qlora_model(
        self, base_model_name: str, adapter_path: str
    ) -> tuple:
        """
        í•™ìŠµëœ QLoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            base_model_name: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„
            adapter_path: ì–´ëŒ‘í„° ê²½ë¡œ

        Returns:
            (model, tokenizer) íŠœí”Œ
        """
        try:
            import torch
            from peft import PeftModel
            from transformers import (
                AutoModelForCausalLM,
                AutoTokenizer,
                BitsAndBytesConfig,
            )
        except ImportError as e:
            raise ImportError(
                f"QLoRA ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: "
                f"pip install torch transformers peft bitsandbytes accelerate\n"
                f"Error: {e}"
            )

        # BitsAndBytes ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )

        # ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(model, adapter_path)

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
        tokenizer.pad_token = tokenizer.eos_token

        print(f"âœ… QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {adapter_path}")

        return model, tokenizer
