"""
RAG ì„œë¹„ìŠ¤

RAG(Retrieval-Augmented Generation) ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

ğŸ˜ğŸ˜ rag_service.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„:

ë²¡í„° ê²€ìƒ‰,

LLM í˜¸ì¶œ,

ì‘ë‹µ í›„ì²˜ë¦¬ê¹Œì§€ ë‹´ë‹¹.

rag_chain.pyë¥¼ ì‹¤ì œë¡œ í˜¸ì¶œí•˜ëŠ” â€œì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë¹„ìŠ¤â€.

"""

from typing import List, Optional, Tuple

from langchain_core.documents import Document
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate

from app.models.base import BaseEmbeddings, BaseLLM
from app.repository.base import BaseVectorRepository


class RAGService:
    """RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(
        self,
        llm: BaseLLM,
        embeddings: BaseEmbeddings,
        repository: BaseVectorRepository,
        similarity_threshold: float = 0.5,
    ):
        """
        RAG ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            llm: LLM ëª¨ë¸ ì œê³µì
            embeddings: Embeddings ëª¨ë¸ ì œê³µì
            repository: ë²¡í„° ìŠ¤í† ì–´ Repository
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        self.llm = llm
        self.embeddings = embeddings
        self.repository = repository
        self.similarity_threshold = similarity_threshold

    def create_rag_prompt(self) -> ChatPromptTemplate:
        """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content="ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”."),
                (
                    "human",
                    """ì°¸ê³  ë¬¸ì„œ:
{context}

{question}""",
                ),
            ]
        )
        return template

    def clean_answer(self, answer) -> str:
        """ë‹µë³€ì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        # ë¨¼ì € ë¬¸ìì—´ë¡œ ë³€í™˜
        if not isinstance(answer, str):
            answer = str(answer)

        # ì œê±°í•  íŒ¨í„´ë“¤ (ë” ë§ì´ ì¶”ê°€)
        patterns = [
            "System:",
            "ì‹œìŠ¤í…œ:",
            "Human:",
            "Answer:",
            "ë‹µë³€:",
            "ì§ˆë¬¸:",
            "ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
            "ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.",
            "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
            "ì°¸ê³  ë¬¸ì„œ:",
            "H:",
            "A:",
        ]

        result = answer.strip()

        # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        prompt_indicators = [
            "ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”",
            "ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”",
            "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ",
        ]

        # í”„ë¡¬í”„íŠ¸ê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        for indicator in prompt_indicators:
            if indicator in result:
                parts = result.split(indicator, 1)
                if len(parts) > 1:
                    result = parts[1].strip()

        # ê° ì¤„ì—ì„œ íŒ¨í„´ì„ ì°¾ì•„ì„œ ì œê±° (ë¼ì¸ì€ ìœ ì§€)
        lines = result.split("\n")
        cleaned_lines = []

        for line in lines:
            cleaned_line = line.strip()

            # ì™„ì „íˆ íŒ¨í„´ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            skip_line = False
            for pattern in patterns:
                if cleaned_line == pattern.rstrip(":").rstrip("?").rstrip("."):
                    skip_line = True
                    break

            if skip_line:
                continue

            # ë¼ì¸ ì‹œì‘ ë¶€ë¶„ì˜ íŒ¨í„´ë§Œ ì œê±°
            for pattern in patterns:
                if cleaned_line.startswith(pattern):
                    cleaned_line = cleaned_line[len(pattern) :].strip()
                    break

            # ë¹ˆ ì¤„ì´ ì•„ë‹ˆë©´ ì¶”ê°€
            if cleaned_line:
                cleaned_lines.append(cleaned_line)

        result = "\n".join(cleaned_lines)

        return result

    def search_relevant_documents(
        self, query: str, k: int = 3
    ) -> List[Tuple[Document, float]]:
        """
        ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

        Returns:
            (ë¬¸ì„œ, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        docs_with_scores = self.repository.search_with_score(query, k=k)
        relevant_docs = [
            (doc, score)
            for doc, score in docs_with_scores
            if score <= self.similarity_threshold
        ]
        return relevant_docs

    def generate_answer(self, question: str, context: Optional[str] = None) -> str:
        """
        ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì»¨í…ìŠ¤íŠ¸ (Noneì´ë©´ ì¼ë°˜ ëŒ€í™”)

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        if context:
            # RAG ëª¨ë“œ
            prompt_template = self.create_rag_prompt()
            prompt = prompt_template.format_messages(context=context, question=question)
        else:
            # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
            general_prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(content="ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
                    ("human", "{question}"),
                ]
            )
            prompt = general_prompt.format_messages(question=question)

        chat_model = self.llm.get_model()
        response = chat_model.invoke(prompt)

        # contentê°€ strì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if isinstance(response.content, str):
            answer = response.content
        else:
            answer = str(response.content)

        # ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
        answer = self.clean_answer(answer)

        return answer
