"""
FastAPI ì„œë²„ - RAG ì±—ë´‡ API

ì›¹ UIì™€ í†µì‹ í•˜ê¸° ìœ„í•œ REST API ì„œë²„ì…ë‹ˆë‹¤.

FastAPI ê¸°ë°˜ RAG ë°±ì—”ë“œ ì„œë²„.

FastAPI ì•±ì˜ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.
í•˜ëŠ” ì¼:
FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±.
router/ ì•ˆì˜ ë¼ìš°í„°ë“¤ include.
CORS, ë¯¸ë“¤ì›¨ì–´, ë¡œê¹…, ì˜ˆì™¸ í•¸ë“¤ëŸ¬ ì„¤ì •.
ì•± ì‹œì‘ ì‹œì ì—:
LLM/ë²¡í„°ìŠ¤í† ì–´/í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë“¤ì„ ë¯¸ë¦¬ ë¡œë“œ(ì˜µì…˜),
ë˜ëŠ” DI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”.
ì—­í• : ì´ í”„ë¡œì íŠ¸ì˜ â€œë©”ì¸ ì‹¤í–‰ íŒŒì¼â€ì´ì API ì„œë²„ì˜ ë¶€íŒ… ìŠ¤í¬ë¦½íŠ¸.

ì´ ëª¨ë“ˆì€ ìˆœìˆ˜í•˜ê²Œ API ì„œë²„ ì—­í• ë§Œ ìˆ˜í–‰í•˜ë©°,
Next.js í”„ë¡ íŠ¸ì—”ë“œ(`frontend/`)ì™€ëŠ” HTTP ìš”ì²­/ì‘ë‹µìœ¼ë¡œë§Œ í†µì‹ í•©ë‹ˆë‹¤.

"""

import os
from datetime import datetime
from typing import List, Optional

from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ)
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_postgres import PGVector
from pydantic import BaseModel


# Pydantic ëª¨ë¸
class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­ ëª¨ë¸"""

    message: str
    model: str = "openai"  # "openai" ë˜ëŠ” "midm"
    session_id: Optional[str] = None


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ ëª¨ë¸"""

    answer: str
    sources: List[str]
    timestamp: str


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ëª¨ë¸"""

    status: str
    message: str


# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="LangChain RAG Chatbot API",
    description="RAGë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• ì±—ë´‡ API",
    version="1.0.0",
)

# CORS ì„¤ì • (ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©í•˜ì„¸ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
vector_store = None
openai_model = None
midm_model = None
embeddings = None


def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (OpenAI + Midm ëª¨ë‘ ë¡œë“œ)."""
    global vector_store, openai_model, midm_model, embeddings

    # Embeddings ì´ˆê¸°í™” (OpenAI ì‚¬ìš©)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. Embeddingsë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.")

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)

    # Neon PGVector ì´ˆê¸°í™”
    db_user = os.getenv("POSTGRES_USER", "neondb_owner")
    db_password = os.getenv("POSTGRES_PASSWORD", "npg_VhUdLOR8F7MQ")
    db_host = os.getenv(
        "POSTGRES_HOST", "ep-mute-boat-a1sgw2su-pooler.ap-southeast-1.aws.neon.tech"
    )
    db_port = os.getenv("POSTGRES_PORT", "5432")
    db_name = os.getenv("POSTGRES_DB", "neondb")
    db_sslmode = os.getenv("POSTGRES_SSLMODE", "require")

    # Neon PostgreSQLì€ SSLì´ í•„ìˆ˜ì…ë‹ˆë‹¤
    connection_string = (
        f"postgresql+psycopg2://{db_user}:{db_password}"
        f"@{db_host}:{db_port}/{db_name}?sslmode={db_sslmode}"
    )

    print("ğŸ”— Neon PostgreSQLì— ì—°ê²° ì¤‘...")
    print(f"   í˜¸ìŠ¤íŠ¸: {db_host}")
    print(f"   ë°ì´í„°ë² ì´ìŠ¤: {db_name}")

    vector_store = PGVector(
        embeddings=embeddings,
        collection_name="langchain_knowledge_base",
        connection=connection_string,
        use_jsonb=True,
    )

    print("âœ… Neon PostgreSQL ì—°ê²° ì™„ë£Œ!")

    # 1. OpenAI ëª¨ë¸ ë¡œë“œ (í•­ìƒ ë¡œë“œ)
    print("\nğŸ”„ [1/2] OpenAI ëª¨ë¸ ë¡œë“œ ì¤‘...")
    openai_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, api_key=api_key)
    print("âœ… OpenAI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # 2. Midm ëª¨ë¸ ë¡œë“œ (í•­ìƒ ë¡œë“œ)
    print("\nğŸ”„ [2/2] Midm ëª¨ë¸ ë¡œë“œ ì¤‘ (GPU + 4bit ì–‘ìí™”)...")
    try:
        import torch
        from langchain_huggingface import HuggingFacePipeline
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            BitsAndBytesConfig,
            pipeline,
        )

        model_path = os.getenv("MIDM_MODEL_PATH", "models/midm")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")

        # GPU í™•ì¸
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   CUDA ë²„ì „: {torch.version.cuda}")
        else:
            print("   âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

        # 4bit ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        print("   4bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ")

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True,
            low_cpu_mem_usage=True,
        )
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, local_files_only=True
        )

        print("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            repetition_penalty=1.1,
        )

        midm_model = HuggingFacePipeline(pipeline=pipe)
        print("âœ… Midm ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (4bit ì–‘ìí™”)!")

    except Exception as e:
        print(f"âŒ Midm ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        print("âš ï¸  Midmì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAIë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        midm_model = None

    print("\nâœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"   - OpenAI: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if openai_model else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    print(f"   - Midm: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if midm_model else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    initialize_rag_system()
    print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")


def create_rag_prompt():
    """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤ (PGVector ë¬¸ì„œ ê¸°ë°˜)."""
    template = ChatPromptTemplate.from_messages(
        [
            SystemMessage(content="ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”."),
            (
                "human",
                """ì°¸ê³  ë¬¸ì„œ:
{context}

{question}""",
            ),
        ]
    )

    return template


def clean_answer(answer) -> str:
    """ë‹µë³€ì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    # ë¨¼ì € ë¬¸ìì—´ë¡œ ë³€í™˜
    if not isinstance(answer, str):
        answer = str(answer)

    # ì œê±°í•  íŒ¨í„´ë“¤ (ë” ë§ì´ ì¶”ê°€)
    patterns = [
        "System:",
        "ì‹œìŠ¤í…œ:",
        "Human:",
        "Answer:",
        "ë‹µë³€:",
        "ì§ˆë¬¸:",
        "ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
        "ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.",
        "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
        "ì°¸ê³  ë¬¸ì„œ:",
        "H:",
        "A:",
    ]

    result = answer.strip()

    # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
    prompt_indicators = [
        "ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”",
        "ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”",
        "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ",
        "RAGê°€ ë­ê³  ì–´ë–»ê²Œ ì‘ë™í•´?",  # ì§ˆë¬¸ ìì²´ë„ ì œê±°
    ]

    # í”„ë¡¬í”„íŠ¸ê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    for indicator in prompt_indicators:
        if indicator in result:
            parts = result.split(indicator, 1)
            if len(parts) > 1:
                result = parts[1].strip()

    # ê° ì¤„ì—ì„œ íŒ¨í„´ì„ ì°¾ì•„ì„œ ì œê±° (ë¼ì¸ì€ ìœ ì§€)
    lines = result.split("\n")
    cleaned_lines = []

    for line in lines:
        cleaned_line = line.strip()

        # ì™„ì „íˆ íŒ¨í„´ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
        skip_line = False
        for pattern in patterns:
            if cleaned_line == pattern.rstrip(":").rstrip("?").rstrip("."):
                skip_line = True
                break

        if skip_line:
            continue

        # ë¼ì¸ ì‹œì‘ ë¶€ë¶„ì˜ íŒ¨í„´ë§Œ ì œê±°
        for pattern in patterns:
            if cleaned_line.startswith(pattern):
                cleaned_line = cleaned_line[len(pattern) :].strip()
                break

        # ë¹ˆ ì¤„ì´ ì•„ë‹ˆë©´ ì¶”ê°€
        if cleaned_line:
            cleaned_lines.append(cleaned_line)

    result = "\n".join(cleaned_lines)

    return result


@app.get("/")
async def root():
    """API ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - API ì •ë³´ ë°˜í™˜"""
    return {
        "name": "LangChain RAG Chatbot API",
        "version": "1.0.0",
        "description": "RAGë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• ì±—ë´‡ API",
        "endpoints": {
            "health": "/health",
            "chat_rag": "/api/chat/rag",
            "chat_general": "/api/chat/general",
            "chat_legacy": "/api/chat",
        },
        "docs": "/docs",
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return HealthResponse(status="healthy", message="RAG Chatbot API is running")


@app.post("/api/chat/rag", response_model=ChatResponse)
async def chat_rag(request: ChatRequest):
    """RAG ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (PGVector + OpenAI/Midm)"""
    try:
        # ëª¨ë¸ ì„ íƒ
        if request.model == "midm":
            if midm_model is None:
                raise HTTPException(
                    status_code=503, detail="Midm ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            selected_model = midm_model
            model_name = "Midm-2.0-Mini-Instruct"
        else:  # "openai"
            if openai_model is None:
                raise HTTPException(
                    status_code=503, detail="OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            selected_model = openai_model
            model_name = "OpenAI GPT-4o-mini"

        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {model_name}")

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        docs_with_scores = vector_store.similarity_search_with_score(
            request.message, k=3
        )

        # ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨ - cosine distance ê¸°ì¤€)
        # pgvectorëŠ” cosine distanceë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë‚®ì„ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŒ
        SIMILARITY_THRESHOLD = 0.5  # 0.5 ì´í•˜ë©´ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨

        # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
        relevant_docs = [
            (doc, score)
            for doc, score in docs_with_scores
            if score <= SIMILARITY_THRESHOLD
        ]

        if not relevant_docs:
            # PGVectorì— ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
            general_prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(content="ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
                    ("human", "{question}"),
                ]
            )

            prompt = general_prompt.format_messages(question=request.message)
            response = selected_model.invoke(prompt)

            # HuggingFacePipelineì€ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ê³ , ChatOpenAIëŠ” ê°ì²´ë¥¼ ë°˜í™˜
            if isinstance(response, str):
                answer = response
            else:
                answer = response.content

            # ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
            answer = clean_answer(answer)

            return ChatResponse(
                answer=answer,
                sources=[f"ğŸ’¬ ì¶œì²˜: {model_name} (ì§€ì‹ ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)"],
                timestamp=datetime.now().isoformat(),
            )

        # PGVectorì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìœ¼ë©´ RAG ì‚¬ìš©
        docs = [doc for doc, score in relevant_docs]

        # ë¬¸ì„œ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        context = "\n\n---\n\n".join([doc.page_content for doc in docs])

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_template = create_rag_prompt()
        prompt = prompt_template.format_messages(
            context=context, question=request.message
        )

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        response = selected_model.invoke(prompt)

        # HuggingFacePipelineì€ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ê³ , ChatOpenAIëŠ” ê°ì²´ë¥¼ ë°˜í™˜
        if isinstance(response, str):
            answer = response
        else:
            answer = response.content

        # ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
        answer = clean_answer(answer)

        # ì§„ì§œ ì¶œì²˜ í‘œì‹œ: PGVectorì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë‘˜ ë‹¤ ì‚¬ìš©
        sources = [f"ğŸ“š ì¶œì²˜: Neon PGVector DB + {model_name}"]
        for doc, score in relevant_docs:
            preview = doc.page_content[:80].replace("\n", " ").strip()
            if len(doc.page_content) > 80:
                preview += "..."
            sources.append(f"{preview} (ìœ ì‚¬ë„: {1 - score:.2f})")

        return ChatResponse(
            answer=answer, sources=sources, timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat/general", response_model=ChatResponse)
async def chat_general(request: ChatRequest):
    """ì¼ë°˜ ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸ (OpenAI/Midm ì„ íƒ, DB ê²€ìƒ‰ ì—†ìŒ)"""
    try:
        # ëª¨ë¸ ì„ íƒ
        if request.model == "midm":
            if midm_model is None:
                raise HTTPException(
                    status_code=503, detail="Midm ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            selected_model = midm_model
            model_name = "Midm-2.0-Mini-Instruct"
        else:  # "openai"
            if openai_model is None:
                raise HTTPException(
                    status_code=503, detail="OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            selected_model = openai_model
            model_name = "OpenAI GPT-4o-mini"

        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {model_name}")

        # ì¼ë°˜ ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸
        general_prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content="ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
                ("human", "{question}"),
            ]
        )

        prompt = general_prompt.format_messages(question=request.message)

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (DB ê²€ìƒ‰ ì—†ì´)
        response = selected_model.invoke(prompt)

        # HuggingFacePipelineì€ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ê³ , ChatOpenAIëŠ” ê°ì²´ë¥¼ ë°˜í™˜
        if isinstance(response, str):
            answer = response
        else:
            answer = response.content

        # ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
        answer = clean_answer(answer)

        return ChatResponse(
            answer=answer,
            sources=[f"ğŸ’¬ ì¶œì²˜: {model_name} (ì¼ë°˜ ëŒ€í™” ëª¨ë“œ)"],
            timestamp=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat", response_model=ChatResponse)
async def chat_legacy(request: ChatRequest):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ (RAGë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)"""
    return await chat_rag(request)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
