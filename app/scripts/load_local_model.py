"""
ë¡œì»¬ Midm ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Midm-2.0-Mini-Instruct ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer


def load_midm_model(model_path: str = "app/models/midm"):
    """
    Midm ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ

    Returns:
        (model, tokenizer) íŠœí”Œ
    """
    print("=" * 70)
    print("ğŸ¤– Midm-2.0-Mini-Instruct ëª¨ë¸ ë¡œë“œ")
    print("=" * 70)
    print(f"\nğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")

    try:
        print("\nğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True  # Mi:dm í•„ìˆ˜
        )

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("\nğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")

        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        print("\n" + "=" * 70)
        print("ğŸ“Š ëª¨ë¸ ì •ë³´")
        print("=" * 70)
        print(f"ëª¨ë¸ íƒ€ì…: {model.config.model_type}")
        print(f"Hidden size: {model.config.hidden_size}")
        print(f"ë ˆì´ì–´ ìˆ˜: {model.config.num_hidden_layers}")
        print(f"ì–´í…ì…˜ í—¤ë“œ: {model.config.num_attention_heads}")
        print(f"Vocabulary size: {model.config.vocab_size}")

        return model, tokenizer

    except Exception as e:
        print(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def test_generation(model, tokenizer, prompt: str = "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?"):
    """
    ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model: ë¡œë“œëœ ëª¨ë¸
        tokenizer: ë¡œë“œëœ í† í¬ë‚˜ì´ì €
        prompt: í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"\nğŸ’¬ í”„ë¡¬í”„íŠ¸: {prompt}")

    try:
        # ì…ë ¥ ì¸ì½”ë”©
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        print("\nğŸ”„ ìƒì„± ì¤‘...")

        # í…ìŠ¤íŠ¸ ìƒì„±
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("\nâœ… ìƒì„± ì™„ë£Œ!")
        print("\n" + "=" * 70)
        print("ğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸")
        print("=" * 70)
        print(generated_text)

    except Exception as e:
        print(f"\nâŒ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_midm_model()

        # í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
        ]

        for prompt in test_prompts:
            test_generation(model, tokenizer, prompt)
            print("\n")

        print("=" * 70)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ìŠ¤í¬ë¦½íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

