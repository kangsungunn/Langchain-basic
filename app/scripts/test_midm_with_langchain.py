"""
LangChainê³¼ Midm ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸

ë¡œì»¬ Midm ëª¨ë¸ì„ LangChainìœ¼ë¡œ ë˜í•‘í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate


def create_midm_llm(model_path: str = "app/models/midm"):
    """
    Midm ëª¨ë¸ì„ LangChain LLMìœ¼ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ

    Returns:
        LangChain HuggingFacePipeline ì¸ìŠ¤í„´ìŠ¤
    """
    print("=" * 70)
    print("ğŸ¤– Midm ëª¨ë¸ì„ LangChainìœ¼ë¡œ ë˜í•‘")
    print("=" * 70)

    try:
        print(f"\nğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")
        print("\nğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True
        )

        tokenizer = AutoTokenizer.from_pretrained(model_path)

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # Pipeline ìƒì„±
        print("\nğŸ”„ Pipeline ìƒì„± ì¤‘...")
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
        )

        # LangChain ë˜í¼ë¡œ ë³€í™˜
        llm = HuggingFacePipeline(pipeline=pipe)

        print("âœ… LangChain ë˜í•‘ ì™„ë£Œ!")

        return llm

    except Exception as e:
        print(f"\nâŒ ë˜í•‘ ì‹¤íŒ¨: {e}")
        raise


def test_langchain_prompt(llm):
    """
    LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸

    Args:
        llm: LangChain LLM ì¸ìŠ¤í„´ìŠ¤
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    template = """ì§ˆë¬¸: {question}

ë‹µë³€:"""

    prompt = PromptTemplate(
        input_variables=["question"],
        template=template
    )

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    questions = [
        "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "RAGëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]

    for i, question in enumerate(questions, 1):
        print(f"\n[ì§ˆë¬¸ {i}] {question}")
        print("-" * 70)

        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            formatted_prompt = prompt.format(question=question)

            # LLM ì‹¤í–‰
            response = llm.invoke(formatted_prompt)

            print(f"[ë‹µë³€] {response}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")


def test_with_custom_llm_wrapper():
    """
    CustomLLMìœ¼ë¡œ ë˜í•‘í•˜ì—¬ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª CustomLLM ë˜í¼ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    try:
        from app.models.providers.custom_provider import CustomLLM

        # LangChain LLM ìƒì„±
        llm_pipeline = create_midm_llm()

        # CustomLLMìœ¼ë¡œ ë˜í•‘
        custom_llm = CustomLLM(
            model=llm_pipeline,
            model_name="midm-2.0-mini"
        )

        print("\nâœ… CustomLLM ë˜í•‘ ì™„ë£Œ!")
        print(f"   ëª¨ë¸ ì´ë¦„: {custom_llm.get_model_name()}")
        print(f"   ì„¤ì •: {custom_llm.get_model_config()}")

        # í…ŒìŠ¤íŠ¸
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."
        print(f"\nğŸ’¬ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {test_prompt}")

        model = custom_llm.get_model()
        response = model.invoke(test_prompt)

        print(f"\nğŸ“ ì‘ë‹µ: {response}")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # 1. LangChain LLM ìƒì„±
        llm = create_midm_llm()

        # 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸
        test_langchain_prompt(llm)

        # 3. CustomLLM ë˜í¼ í…ŒìŠ¤íŠ¸
        test_with_custom_llm_wrapper()

        print("\n" + "=" * 70)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ìŠ¤í¬ë¦½íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

