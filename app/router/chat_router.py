"""
ì±„íŒ… ë¼ìš°í„°

ì±„íŒ… ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.


ğŸ˜ğŸ˜ FastAPI ê¸°ì¤€ì˜ API ì—”ë“œí¬ì¸íŠ¸ ê³„ì¸µì…ë‹ˆë‹¤.

chat_router.py
POST /api/chat
ì„¸ì…˜ ID, ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ë“±ì„ ë°›ì•„ ëŒ€í™”í˜• ì‘ë‹µ ë°˜í™˜.

QLoRA íŒŒì¸íŠœë‹ ë° ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸ í¬í•¨.

"""

from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

from app.api.dependencies import get_chat_service
from app.services.chat_service import ChatService

# QLoRA ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ì „ì—­ ë³€ìˆ˜
_qlora_model = None
_qlora_tokenizer = None
_qlora_model_info = {"loaded": False, "model_name": None, "adapter_path": None}


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­ ëª¨ë¸"""

    message: str
    model: Optional[str] = None  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œ (í˜„ì¬ ë¯¸ì‚¬ìš©)


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ ëª¨ë¸"""

    answer: str
    sources: List[str]
    timestamp: str
    model_info: Optional[Dict[str, Any]] = None


class QLoRALoadRequest(BaseModel):
    """QLoRA ëª¨ë¸ ë¡œë“œ ìš”ì²­"""

    model_name: str = "beomi/Llama-3-Open-Ko-8B"
    lora_r: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    target_modules: Optional[List[str]] = None


class QLoRATrainRequest(BaseModel):
    """QLoRA í•™ìŠµ ìš”ì²­"""

    conversations: List[Dict[str, str]]
    output_dir: str = "./checkpoints/qlora"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    learning_rate: float = 2e-4


class QLoRAStatusResponse(BaseModel):
    """QLoRA ëª¨ë¸ ìƒíƒœ ì‘ë‹µ"""

    loaded: bool
    model_name: Optional[str] = None
    adapter_path: Optional[str] = None


chat_router = APIRouter(prefix="/api/chat", tags=["chat"])


@chat_router.post("/rag", response_model=ChatResponse)
async def chat_rag(
    request: ChatRequest, chat_service: ChatService = Depends(get_chat_service)
):
    """
    RAG ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Vector DB + LLM)

    ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³ , LLMìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        result = chat_service.chat_rag(request.message)
        return ChatResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@chat_router.post("/general", response_model=ChatResponse)
async def chat_general(
    request: ChatRequest, chat_service: ChatService = Depends(get_chat_service)
):
    """
    ì¼ë°˜ ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸ (LLMë§Œ ì‚¬ìš©)

    ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ì—†ì´ LLMë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        result = chat_service.chat_general(request.message)
        return ChatResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@chat_router.post("", response_model=ChatResponse)
async def chat_legacy(
    request: ChatRequest, chat_service: ChatService = Depends(get_chat_service)
):
    """
    ë ˆê±°ì‹œ ì—”ë“œí¬ì¸íŠ¸ (RAGë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)

    ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.
    """
    return await chat_rag(request, chat_service)


# ==================== QLoRA ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ====================


@chat_router.post("/qlora/load")
async def load_qlora_model(
    request: QLoRALoadRequest, chat_service: ChatService = Depends(get_chat_service)
):
    """
    QLoRA ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë² ì´ìŠ¤ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    global _qlora_model, _qlora_tokenizer, _qlora_model_info

    try:
        print(f"ğŸ”„ QLoRA ëª¨ë¸ ë¡œë“œ ì¤‘: {request.model_name}")

        model, tokenizer = chat_service.load_qlora_model(
            model_name=request.model_name,
            lora_r=request.lora_r,
            lora_alpha=request.lora_alpha,
            lora_dropout=request.lora_dropout,
            target_modules=request.target_modules,
        )

        _qlora_model = model
        _qlora_tokenizer = tokenizer
        _qlora_model_info = {
            "loaded": True,
            "model_name": request.model_name,
            "adapter_path": None,
            "config": {
                "lora_r": request.lora_r,
                "lora_alpha": request.lora_alpha,
                "lora_dropout": request.lora_dropout,
            },
        }

        return {
            "status": "success",
            "message": f"QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {request.model_name}",
            "model_info": _qlora_model_info,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@chat_router.post("/qlora/load_trained")
async def load_trained_qlora_model(
    base_model_name: str,
    adapter_path: str,
    chat_service: ChatService = Depends(get_chat_service),
):
    """
    í•™ìŠµëœ QLoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    ì´ë¯¸ íŒŒì¸íŠœë‹ì´ ì™„ë£Œëœ ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë¡œë“œí•©ë‹ˆë‹¤.
    """
    global _qlora_model, _qlora_tokenizer, _qlora_model_info

    try:
        print(f"ğŸ”„ í•™ìŠµëœ QLoRA ëª¨ë¸ ë¡œë“œ ì¤‘: {adapter_path}")

        model, tokenizer = chat_service.load_trained_qlora_model(
            base_model_name=base_model_name, adapter_path=adapter_path
        )

        _qlora_model = model
        _qlora_tokenizer = tokenizer
        _qlora_model_info = {
            "loaded": True,
            "model_name": base_model_name,
            "adapter_path": adapter_path,
        }

        return {
            "status": "success",
            "message": f"í•™ìŠµëœ QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {adapter_path}",
            "model_info": _qlora_model_info,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@chat_router.get("/qlora/status", response_model=QLoRAStatusResponse)
async def get_qlora_status():
    """
    í˜„ì¬ ë¡œë“œëœ QLoRA ëª¨ë¸ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    return QLoRAStatusResponse(**_qlora_model_info)


@chat_router.post("/qlora/chat", response_model=ChatResponse)
async def chat_with_qlora(
    request: ChatRequest,
    max_new_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    chat_service: ChatService = Depends(get_chat_service),
):
    """
    ë¡œë“œëœ QLoRA ëª¨ë¸ê³¼ ëŒ€í™”í•©ë‹ˆë‹¤.

    ë¨¼ì € /qlora/load ë˜ëŠ” /qlora/load_trainedë¡œ ëª¨ë¸ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    global _qlora_model, _qlora_tokenizer

    if _qlora_model is None or _qlora_tokenizer is None:
        raise HTTPException(
            status_code=400,
            detail="QLoRA ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € /qlora/loadë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.",
        )

    try:
        result = chat_service.chat_with_qlora_model(
            model=_qlora_model,
            tokenizer=_qlora_tokenizer,
            message=request.message,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        return ChatResponse(**result)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")


@chat_router.post("/qlora/train")
async def train_qlora_model(
    request: QLoRATrainRequest, chat_service: ChatService = Depends(get_chat_service)
):
    """
    QLoRA ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

    ë¨¼ì € /qlora/loadë¡œ ëª¨ë¸ì„ ë¡œë“œí•œ í›„, ì´ ì—”ë“œí¬ì¸íŠ¸ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.
    í•™ìŠµ ë°ì´í„°ëŠ” [{"prompt": "ì§ˆë¬¸", "response": "ë‹µë³€"}] í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    global _qlora_model, _qlora_tokenizer

    if _qlora_model is None or _qlora_tokenizer is None:
        raise HTTPException(
            status_code=400,
            detail="QLoRA ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € /qlora/loadë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.",
        )

    try:
        # í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
        train_dataset = chat_service.prepare_training_dataset(
            tokenizer=_qlora_tokenizer, conversations=request.conversations
        )

        # í•™ìŠµ ì‹œì‘
        result = chat_service.train_qlora_model(
            model=_qlora_model,
            tokenizer=_qlora_tokenizer,
            train_dataset=train_dataset,
            output_dir=request.output_dir,
            num_train_epochs=request.num_train_epochs,
            per_device_train_batch_size=request.per_device_train_batch_size,
            learning_rate=request.learning_rate,
        )

        # í•™ìŠµ í›„ ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
        _qlora_model_info["adapter_path"] = result["final_model_path"]

        return {"status": "success", "message": "QLoRA í•™ìŠµ ì™„ë£Œ", "result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")


@chat_router.post("/qlora/unload")
async def unload_qlora_model():
    """
    ë¡œë“œëœ QLoRA ëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ í•´ì œí•©ë‹ˆë‹¤.
    """
    global _qlora_model, _qlora_tokenizer, _qlora_model_info

    if _qlora_model is None:
        return {"status": "success", "message": "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}

    try:
        # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
        del _qlora_model
        del _qlora_tokenizer
        _qlora_model = None
        _qlora_tokenizer = None
        _qlora_model_info = {"loaded": False, "model_name": None, "adapter_path": None}

        # GPU ìºì‹œ ì •ë¦¬
        import torch

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {"status": "success", "message": "QLoRA ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
