"""
ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  PGVectorì— ì €ì¥í•©ë‹ˆë‹¤.
"""
import os
import sys
from typing import List

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter


def print_banner(text: str) -> None:
    """ë°°ë„ˆë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 70)
    print(f"  {text}")
    print("=" * 70)


def load_knowledge_file(file_path: str) -> str:
    """ì§€ì‹ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)


def split_text_into_chunks(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Document]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n## ", "\n### ", "\n\n", "\n", " ", ""],
        length_function=len,
    )

    chunks = text_splitter.split_text(text)

    # Document ê°ì²´ë¡œ ë³€í™˜
    documents = []
    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                "chunk_id": i,
                "source": "knowledge_base",
                "total_chunks": len(chunks)
            }
        )
        documents.append(doc)

    return documents


def build_knowledge_base() -> None:
    """ì§€ì‹ ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    print_banner("ğŸ“š LangChain ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•")

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\nâŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        sys.exit(1)

    # Step 1: ì§€ì‹ íŒŒì¼ ì½ê¸°
    print_banner("Step 1: ì§€ì‹ íŒŒì¼ ì½ê¸°")

    knowledge_file = "/knowledge/sample_knowledge.txt"
    print(f"\nğŸ“– íŒŒì¼ ê²½ë¡œ: {knowledge_file}")

    content = load_knowledge_file(knowledge_file)
    print(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ!")
    print(f"   ì´ ë¬¸ì ìˆ˜: {len(content):,}")
    print(f"   ì´ ì¤„ ìˆ˜: {len(content.splitlines())}")

    # Step 2: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
    print_banner("Step 2: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• ")

    print("\nğŸ”ª í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    print("   ì²­í¬ í¬ê¸°: 500 ë¬¸ì")
    print("   ì¤‘ë³µ ì˜ì—­: 50 ë¬¸ì")
    print("   (ì²­í¬ ê°„ ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´)")

    documents = split_text_into_chunks(content)

    print(f"\nâœ… ë¶„í•  ì™„ë£Œ!")
    print(f"   ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(documents)}")

    # ëª‡ ê°œ ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“„ ìƒ˜í”Œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:")
    for i, doc in enumerate(documents[:3], 1):
        preview = doc.page_content[:100].replace('\n', ' ')
        print(f"   [{i}] {preview}...")

    # Step 3: OpenAI Embeddings ì´ˆê¸°í™”
    print_banner("Step 3: OpenAI Embeddings ì´ˆê¸°í™”")

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=api_key
    )
    print("âœ… OpenAI Embeddings ì´ˆê¸°í™” ì™„ë£Œ")
    print("   ëª¨ë¸: text-embedding-3-small")
    print("   ë²¡í„° ì°¨ì›: 1536")

    # Step 4: PGVector ì´ˆê¸°í™”
    print_banner("Step 4: PGVector ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°")

    db_user = os.getenv("POSTGRES_USER", "langchain")
    db_password = os.getenv("POSTGRES_PASSWORD", "langchain123")
    db_host = os.getenv("POSTGRES_HOST", "localhost")
    db_port = os.getenv("POSTGRES_PORT", "5432")
    db_name = os.getenv("POSTGRES_DB", "vectordb")

    connection_string = (
        f"postgresql+psycopg2://{db_user}:{db_password}"
        f"@{db_host}:{db_port}/{db_name}"
    )

    collection_name = "langchain_knowledge_base"

    print(f"\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
    print(f"   Host: {db_host}:{db_port}")
    print(f"   Database: {db_name}")
    print(f"   Collection: {collection_name}")

    vector_store = PGVector(
        embeddings=embeddings,
        collection_name=collection_name,
        connection=connection_string,
        use_jsonb=True,
    )

    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")

    # Step 5: ë²¡í„° ì €ì¥
    print_banner("Step 5: ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥")

    print(f"\nğŸ“¤ {len(documents)}ê°œì˜ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ ì¤‘...")
    print("   (OpenAI API í˜¸ì¶œ ì¤‘ - ìˆ˜ ì´ˆ ì†Œìš”)")
    print("   â³ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")

    try:
        ids = vector_store.add_documents(documents)
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ!")
        print(f"   ì €ì¥ëœ ë¬¸ì„œ ID: {len(ids)}ê°œ")
        print(f"   ì²« 3ê°œ ID: {ids[:3]}")
    except Exception as e:
        print(f"\nâŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # Step 6: ì €ì¥ í™•ì¸ (í…ŒìŠ¤íŠ¸ ê²€ìƒ‰)
    print_banner("Step 6: ì €ì¥ í™•ì¸ (í…ŒìŠ¤íŠ¸ ê²€ìƒ‰)")

    test_queries = [
        "LangChainì´ ë­ì•¼?",
        "RAGëŠ” ì–´ë–»ê²Œ ì‘ë™í•´?",
        "PGVectorì˜ ì¥ì ì€?",
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}: '{query}'")

        results = vector_store.similarity_search(query, k=1)

        if results:
            preview = results[0].page_content[:150].replace('\n', ' ')
            print(f"   âœ… ê´€ë ¨ ë¬¸ì„œ ì°¾ìŒ:")
            print(f"      {preview}...")
        else:
            print(f"   âš ï¸  ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # Step 7: ë¹„ìš© ì •ë³´
    print_banner("ğŸ’° ì˜ˆìƒ ë¹„ìš©")

    total_chars = sum(len(doc.page_content) for doc in documents)
    estimated_tokens = int(total_chars * 1.3)  # ë¬¸ìë¥¼ í† í°ìœ¼ë¡œ ëŒ€ëµ ë³€í™˜
    cost = (estimated_tokens / 1_000_000) * 0.02

    print(f"\nğŸ“Š ì²˜ë¦¬ ì •ë³´:")
    print(f"   ì´ ë¬¸ì ìˆ˜: {total_chars:,}")
    print(f"   ì˜ˆìƒ í† í° ìˆ˜: {estimated_tokens:,}")
    print(f"   ì˜ˆìƒ ë¹„ìš©: ${cost:.6f} (ì•½ {cost * 1300:.2f}ì›)")
    print("\nğŸ’¡ ë§¤ìš° ì €ë ´í•©ë‹ˆë‹¤!")

    # ìµœì¢… ë©”ì‹œì§€
    print_banner("âœ… ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

    print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤!")
    print(f"\nğŸ“Š êµ¬ì¶• ê²°ê³¼:")
    print(f"   â€¢ Collection: {collection_name}")
    print(f"   â€¢ ì €ì¥ëœ ì²­í¬: {len(documents)}ê°œ")
    print(f"   â€¢ ë²¡í„° ì°¨ì›: 1536")
    print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: text-embedding-3-small")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"   â†’ RAG ì±—ë´‡ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°")
    print(f"   â†’ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ì •í™•í•œ ë‹µë³€ ìƒì„±")
    print()


if __name__ == "__main__":
    try:
        build_knowledge_base()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

