"""
Midm ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸

í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""
import os

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["LLM_PROVIDER"] = "local_llama"
os.environ["MIDM_MODEL_PATH"] = "app/models/midm"

print("=" * 70)
print("ğŸ§ª Midm ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
print("=" * 70)

print(f"\ní™˜ê²½ ë³€ìˆ˜:")
print(f"  LLM_PROVIDER: {os.getenv('LLM_PROVIDER')}")
print(f"  MIDM_MODEL_PATH: {os.getenv('MIDM_MODEL_PATH')}")

try:
    print("\nğŸ”„ ëª¨ë¸ íŒ©í† ë¦¬ì—ì„œ LLM ìƒì„± ì¤‘...")
    from app.models.factory import ModelFactory

    llm = ModelFactory.create_llm()

    print(f"âœ… LLM ìƒì„± ì™„ë£Œ!")
    print(f"   ëª¨ë¸ ì´ë¦„: {llm.get_model_name()}")
    print(f"   ëª¨ë¸ ì„¤ì •: {llm.get_model_config()}")

    print("\nğŸ”„ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    model = llm.get_model()

    print(f"âœ… ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
    print(f"   íƒ€ì…: {type(model)}")

    print("\nğŸ§ª ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸...")
    response = model.invoke("ì•ˆë…•í•˜ì„¸ìš”!")
    print(f"âœ… ì‘ë‹µ: {response}")

    print("\n" + "=" * 70)
    print("âœ… Midm ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()

