"""
ë¡œì»¬ Llama ëª¨ë¸ ì œê³µì

ë¡œì»¬ì— ì €ì¥ëœ Llama ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""
import os
from typing import Any, Optional

from langchain_core.language_models import BaseChatModel

from app.models.base import BaseLLM


class LocalLlamaLLM(BaseLLM):
    """ë¡œì»¬ Llama ëª¨ë¸ êµ¬í˜„

    HuggingFace Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        model_name: Optional[str] = None,
        device: str = "cpu",
        **kwargs
    ):
        """
        ë¡œì»¬ Llama ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: app/models/midm)
            model_name: ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: local-llama)
            device: ë””ë°”ì´ìŠ¤ (cpu, cuda ë“±)
            **kwargs: ì¶”ê°€ ì„¤ì •

        ì‚¬ìš© ì˜ˆì‹œ:
            # ë°©ë²• 1: HuggingFace Pipeline
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from langchain_huggingface import HuggingFacePipeline

            model = AutoModelForCausalLM.from_pretrained(model_path)
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            pipeline = HuggingFacePipeline(model=model, tokenizer=tokenizer)

            # ë°©ë²• 2: llama.cpp
            from langchain_community.llms import LlamaCpp

            llm = LlamaCpp(model_path="path/to/model.gguf")
        """
        self.model_path = model_path or os.getenv(
            "LOCAL_MODEL_PATH", "app/models/midm"
        )
        self.model_name = model_name or "local-llama"
        self.device = device
        self.kwargs = kwargs

        self._model: Optional[BaseChatModel] = None

    def get_model(self) -> BaseChatModel:
        """
        LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Midm-2.0-Mini-Instruct ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Returns:
            LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if self._model is None:
            try:
                from transformers import (
                    AutoModelForCausalLM,
                    AutoTokenizer,
                    pipeline as hf_pipeline
                )
                from langchain_huggingface import HuggingFacePipeline

                print(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")

                # Midm ëª¨ë¸ ë¡œë“œ
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype="auto",
                    device_map="auto",
                    trust_remote_code=True  # Mi:dm í•„ìˆ˜
                )

                tokenizer = AutoTokenizer.from_pretrained(self.model_path)

                print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
                print(f"   ë””ë°”ì´ìŠ¤: {self.device}")

                # Pipeline ìƒì„±
                pipe = hf_pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_new_tokens=self.kwargs.get("max_new_tokens", 512),
                    temperature=self.kwargs.get("temperature", 0.7),
                    do_sample=True,
                    top_p=self.kwargs.get("top_p", 0.9),
                )

                # LangChain ë˜í¼ë¡œ ë³€í™˜
                self._model = HuggingFacePipeline(pipeline=pipe)

            except ImportError as e:
                raise ImportError(
                    "ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:\n"
                    "pip install transformers torch langchain-huggingface accelerate"
                ) from e
            except Exception as e:
                raise RuntimeError(
                    f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\n"
                    f"ëª¨ë¸ ê²½ë¡œ: {self.model_path}\n"
                    "ëª¨ë¸ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
                ) from e

        return self._model

    def get_model_name(self) -> str:
        """ëª¨ë¸ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.model_name

    def get_model_config(self) -> dict[str, Any]:
        """ëª¨ë¸ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "provider": "local_llama",
            "model": self.model_name,
            "model_path": self.model_path,
            "device": self.device,
            **self.kwargs
        }

