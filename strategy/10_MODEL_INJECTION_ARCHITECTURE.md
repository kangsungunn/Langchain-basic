# π—οΈ λ¨λΈ μ£Όμ… μ•„ν‚¤ν…μ²

## π“ ν΄λ” κµ¬μ΅°

```
app/
β”β”€β”€ models/                    # λ¨λΈ λ μ΄μ–΄
β”‚   β”β”€β”€ __init__.py
β”‚   β”β”€β”€ base.py               # μ¶”μƒ μΈν„°νμ΄μ¤ (BaseLLM, BaseEmbeddings)
β”‚   β”β”€β”€ factory.py            # λ¨λΈ ν©ν† λ¦¬ (μμ΅΄μ„± μ£Όμ…)
β”‚   β””β”€β”€ providers/            # λ¨λΈ μ κ³µμ κµ¬ν„
β”‚       β”β”€β”€ __init__.py
β”‚       β”β”€β”€ openai_provider.py    # OpenAI λ¨λΈ κµ¬ν„
β”‚       β””β”€β”€ custom_provider.py    # μ»¤μ¤ν…€ λ¨λΈ κµ¬ν„ (μ§μ ‘ μ£Όμ…μ©)
β”‚
β”β”€β”€ services/                 # λΉ„μ¦λ‹μ¤ λ΅μ§ λ μ΄μ–΄
β”‚   β”β”€β”€ __init__.py
β”‚   β”β”€β”€ rag_service.py       # RAG μ„λΉ„μ¤
β”‚   β””β”€β”€ chat_service.py      # μ±„ν… μ„λΉ„μ¤
β”‚
β”β”€β”€ api/                      # API λ μ΄μ–΄
β”‚   β”β”€β”€ __init__.py
β”‚   β”β”€β”€ routes.py            # API μ—”λ“ν¬μΈνΈ μ •μ
β”‚   β””β”€β”€ dependencies.py      # FastAPI μμ΅΄μ„± μ£Όμ…
β”‚
β”β”€β”€ config/                   # μ„¤μ • λ μ΄μ–΄
β”‚   β”β”€β”€ __init__.py
β”‚   β””β”€β”€ settings.py          # ν™κ²½ λ³€μ λ° μ„¤μ • κ΄€λ¦¬
β”‚
β”β”€β”€ api_server.py            # κΈ°μ΅΄ API μ„λ²„ (λ κ±°μ‹)
β”β”€β”€ api_server_refactored.py # λ¦¬ν©ν† λ§λ API μ„λ²„
β””β”€β”€ ...                      # κΈ°νƒ€ μ¤ν¬λ¦½νΈλ“¤
```

## π”„ μμ΅΄μ„± μ£Όμ… νλ¦„

```
FastAPI App
    β†“
API Routes (routes.py)
    β†“ (μμ΅΄μ„± μ£Όμ…)
Dependencies (dependencies.py)
    β†“
Services (rag_service.py, chat_service.py)
    β†“
Models (factory.py β†’ providers/)
```

## π― λ¨λΈ μ£Όμ… λ°©λ²•

### λ°©λ²• 1: ν™κ²½ λ³€μ μ‚¬μ© (κΈ°λ³Έ)

```bash
# .env νμΌ λλ” ν™κ²½ λ³€μ
LLM_PROVIDER=openai
OPENAI_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_key_here

EMBEDDINGS_PROVIDER=openai
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small
```

### λ°©λ²• 2: μ»¤μ¤ν…€ λ¨λΈ μ§μ ‘ μ£Όμ…

`app/api/dependencies.py`μ `get_llm()` ν•¨μλ¥Ό μμ •:

```python
from app.models.providers.custom_provider import CustomLLM
from langchain_ollama import ChatOllama  # μμ‹

def get_llm() -> BaseLLM:
    # μ»¤μ¤ν…€ λ¨λΈ μƒμ„±
    custom_model = ChatOllama(model="llama2")

    # CustomLLMμΌλ΅ λν•‘
    return CustomLLM(model=custom_model, model_name="llama2")
```

### λ°©λ²• 3: ν©ν† λ¦¬ ν¨ν„΄ ν™•μ¥

μƒλ΅μ΄ μ κ³µμλ¥Ό μ¶”κ°€ν•λ ¤λ©΄:

1. `app/models/providers/`μ— μƒ μ κ³µμ νμΌ μƒμ„±
2. `BaseLLM` λλ” `BaseEmbeddings` κµ¬ν„
3. `app/models/factory.py`μ `ModelFactory`μ— μ¶”κ°€

## π“ μ£Όμ” μ»΄ν¬λ„νΈ

### 1. Models (`app/models/`)

- **λ©μ **: LLMκ³Ό Embeddings λ¨λΈμ μ¶”μƒν™” λ° κ΄€λ¦¬
- **μΈν„°νμ΄μ¤**: `BaseLLM`, `BaseEmbeddings`
- **ν©ν† λ¦¬**: `ModelFactory` - ν™κ²½μ— λ”°λΌ μ μ ν• λ¨λΈ μƒμ„±
- **μ κ³µμ**: OpenAI, Custom λ“± λ‹¤μ–‘ν• λ¨λΈ μ§€μ›

### 2. Services (`app/services/`)

- **RAGService**: RAG λ΅μ§ μ²λ¦¬ (λ¬Έμ„ κ²€μƒ‰, λ‹µλ³€ μƒμ„±)
- **ChatService**: μ±„ν… λΉ„μ¦λ‹μ¤ λ΅μ§ (RAG/μΌλ° λ¨λ“)

### 3. API (`app/api/`)

- **routes.py**: FastAPI μ—”λ“ν¬μΈνΈ μ •μ
- **dependencies.py**: μμ΅΄μ„± μ£Όμ… ν•¨μλ“¤

### 4. Config (`app/config/`)

- **settings.py**: ν™κ²½ λ³€μ κΈ°λ° μ„¤μ • κ΄€λ¦¬

## π€ μ‚¬μ© μμ‹

### λ¦¬ν©ν† λ§λ μ„λ²„ μ‹¤ν–‰

```bash
# api_server_refactored.py μ‚¬μ©
uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000 --reload
```

### μ»¤μ¤ν…€ λ¨λΈ μ£Όμ… μμ‹

```python
# app/api/dependencies.py μμ •
from app.models.providers.custom_provider import CustomLLM
from your_custom_model import YourCustomModel

def get_llm() -> BaseLLM:
    custom_model = YourCustomModel(...)
    return CustomLLM(model=custom_model, model_name="your-model")
```

## π”§ ν™•μ¥ κ°€μ΄λ“

### μƒλ΅μ΄ λ¨λΈ μ κ³µμ μ¶”κ°€

1. `app/models/providers/your_provider.py` μƒμ„±
2. `BaseLLM` λλ” `BaseEmbeddings` κµ¬ν„
3. `app/models/factory.py`μ `ModelFactory`μ— μ¶”κ°€

```python
# app/models/factory.py
elif provider.lower() == "your_provider":
    return YourProviderLLM(model_name=model_name, **kwargs)
```

## β… μ¥μ 

1. **κ΄€μ‹¬μ‚¬μ λ¶„λ¦¬**: κ° λ μ΄μ–΄κ°€ λ…ν™•ν• μ±…μ„μ„ κ°€μ§
2. **μμ΅΄μ„± μ£Όμ…**: λ¨λΈμ„ μ‰½κ² κµμ²΄ κ°€λ¥
3. **ν…μ¤νΈ μ©μ΄**: κ° μ»΄ν¬λ„νΈλ¥Ό λ…λ¦½μ μΌλ΅ ν…μ¤νΈ κ°€λ¥
4. **ν™•μ¥μ„±**: μƒλ΅μ΄ λ¨λΈ μ κ³µμλ¥Ό μ‰½κ² μ¶”κ°€ κ°€λ¥
5. **μ μ§€λ³΄μμ„±**: μ½”λ“ κµ¬μ΅°κ°€ λ…ν™•ν•κ³  μ΄ν•΄ν•κΈ° μ‰¬μ›€

