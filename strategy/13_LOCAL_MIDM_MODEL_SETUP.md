# ğŸ¤– Midm-2.0-Mini-Instruct ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ë¡œì»¬ Midm-2.0-Mini-Instruct ëª¨ë¸ì„ LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ”§ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install transformers torch langchain-huggingface accelerate
```

## ğŸ“‚ ëª¨ë¸ íŒŒì¼ í™•ì¸

ëª¨ë¸ íŒŒì¼ì´ `app/models/midm/` ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸:

```
app/models/midm/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ model.safetensors (4.3GB)
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ special_tokens_map.json
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • (ê¶Œì¥)

```bash
# .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜
export LLM_PROVIDER=local_llama
export MIDM_MODEL_PATH=app/models/midm
export MIDM_DEVICE=auto  # ë˜ëŠ” cpu, cuda
export MIDM_MAX_NEW_TOKENS=512
export MIDM_TEMPERATURE=0.7

# ì„œë²„ ì‹¤í–‰
uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000
```

### ë°©ë²• 2: dependencies.pyì—ì„œ ì§ì ‘ ì„¤ì •

`app/api/dependencies.py` ìˆ˜ì •:

```python
from app.models.providers.local_llama_provider import LocalLlamaLLM

@lru_cache()
def get_llm() -> BaseLLM:
    # Midm ëª¨ë¸ ë¡œë“œ
    return LocalLlamaLLM(
        model_path="app/models/midm",
        model_name="midm-2.0-mini-instruct",
        device="auto",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
    )
```

### ë°©ë²• 3: CustomLLMìœ¼ë¡œ ì§ì ‘ ì£¼ì…

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from app.models.providers.custom_provider import CustomLLM

@lru_cache()
def get_llm() -> BaseLLM:
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        "app/models/midm",
        torch_dtype="auto",
        device_map="auto",
        trust_remote_code=True  # Mi:dm í•„ìˆ˜
    )

    tokenizer = AutoTokenizer.from_pretrained("app/models/midm")

    # Pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
    )

    # LangChain ë˜í¼
    hf_pipeline = HuggingFacePipeline(pipeline=pipe)

    # CustomLLMìœ¼ë¡œ ë˜í•‘
    return CustomLLM(model=hf_pipeline, model_name="midm-2.0-mini")
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸

```bash
python app/scripts/load_local_model.py
```

### 2. LangChain í†µí•© í…ŒìŠ¤íŠ¸

```bash
python app/scripts/test_midm_with_langchain.py
```

### 3. API í…ŒìŠ¤íŠ¸

```bash
# ì„œë²„ ì‹¤í–‰
export LLM_PROVIDER=local_llama
uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/chat/general \
  -H "Content-Type: application/json" \
  -d '{"message": "ì•ˆë…•í•˜ì„¸ìš”!"}'
```

## ğŸ“Š ëª¨ë¸ ì •ë³´

- **ëª¨ë¸ ì´ë¦„**: Midm-2.0-Mini-Instruct
- **ëª¨ë¸ íƒ€ì…**: LlamaForCausalLM
- **ëª¨ë¸ í¬ê¸°**: 4.3GB
- **Hidden size**: 1792
- **ë ˆì´ì–´ ìˆ˜**: 48
- **ì–´í…ì…˜ í—¤ë“œ**: 32
- **Vocabulary size**: 131,392

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

| í™˜ê²½ ë³€ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|--------|------|
| `LLM_PROVIDER` | `openai` | ëª¨ë¸ ì œê³µì (`openai`, `local_llama`) |
| `MIDM_MODEL_PATH` | `app/models/midm` | Midm ëª¨ë¸ ê²½ë¡œ |
| `MIDM_DEVICE` | `auto` | ë””ë°”ì´ìŠ¤ (`auto`, `cpu`, `cuda`) |
| `MIDM_MAX_NEW_TOKENS` | `512` | ìµœëŒ€ ìƒì„± í† í° ìˆ˜ |
| `MIDM_TEMPERATURE` | `0.7` | ì˜¨ë„ (0.0 ~ 1.0) |
| `MIDM_TOP_P` | `0.9` | Top-p ìƒ˜í”Œë§ |

## ğŸ”„ ëª¨ë¸ ì „í™˜

### OpenAI â†” Midm ì „í™˜

```bash
# OpenAI ì‚¬ìš©
export LLM_PROVIDER=openai
export OPENAI_API_KEY=sk-...

# Midm ì‚¬ìš©
export LLM_PROVIDER=local_llama
export MIDM_MODEL_PATH=app/models/midm

# ì„œë²„ ì¬ì‹œì‘
uvicorn app.api_server_refactored:app --reload
```

## ğŸ’¡ ì„±ëŠ¥ ìµœì í™”

### GPU ì‚¬ìš©

```bash
# CUDA ì‚¬ìš© (NVIDIA GPU)
export MIDM_DEVICE=cuda

# íŠ¹ì • GPU ì„ íƒ
export CUDA_VISIBLE_DEVICES=0
```

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
# 8-bit ì–‘ìí™” ì‚¬ìš©
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    load_in_8bit=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    device_map="auto",
    trust_remote_code=True
)
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**: ìµœì†Œ 8GB RAM (CPU), 4GB VRAM (GPU)
2. **trust_remote_code**: Mi:dm ëª¨ë¸ì€ ë°˜ë“œì‹œ `trust_remote_code=True` í•„ìš”
3. **ì†ë„**: CPU ì‚¬ìš© ì‹œ ëŠë¦´ ìˆ˜ ìˆìŒ (GPU ê¶Œì¥)
4. **ì²« ì‹¤í–‰**: ëª¨ë¸ ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ (ìºì‹±ë¨)

## ğŸ“š ê´€ë ¨ íŒŒì¼

- `app/models/providers/local_llama_provider.py` - Midm ëª¨ë¸ ë¡œë”
- `app/scripts/load_local_model.py` - ê¸°ë³¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
- `app/scripts/test_midm_with_langchain.py` - LangChain í†µí•© í…ŒìŠ¤íŠ¸
- `app/config/model_config.py` - ëª¨ë¸ ì„¤ì • ê´€ë¦¬

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ ë¡œë“œ í™•ì¸
2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
3. ì„œë²„ ì‹¤í–‰ ë° API í…ŒìŠ¤íŠ¸
4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

