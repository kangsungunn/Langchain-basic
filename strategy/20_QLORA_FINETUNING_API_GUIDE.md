# 20. QLoRA νμΈνλ‹ λ° λ€ν™” API κ°€μ΄λ“

## π“‹ κ°μ”

μ΄ λ¬Έμ„λ” `chat_service.py`μ™€ `chat_router.py`μ— κµ¬ν„λ QLoRA κΈ°λ° νμΈνλ‹ λ° λ€ν™” κΈ°λ¥μ μ‚¬μ© κ°€μ΄λ“μ…λ‹λ‹¤.

## π― μ£Όμ” κΈ°λ¥

### 1. QLoRA (Quantized Low-Rank Adaptation)
- **4bit μ–‘μν™”**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ λ¨λΈ λ΅λ”©
- **LoRA μ–΄λ‘ν„°**: μΌλ¶€ νλΌλ―Έν„°λ§ ν•™μµν•μ—¬ λΉ λ¥Έ νμΈνλ‹
- **λ©”λ¨λ¦¬ μ μ•½**: λ€ν• LLMμ„ μ†ν• GPUμ—μ„λ„ ν•™μµ κ°€λ¥

### 2. API μ—”λ“ν¬μΈνΈ
- λ¨λΈ λ΅λ“/μ–Έλ΅λ“
- νμΈνλ‹ μ‹¤ν–‰
- ν•™μµλ λ¨λΈκ³Ό λ€ν™”
- λ¨λΈ μƒνƒ κ΄€λ¦¬

## π› οΈ API μ—”λ“ν¬μΈνΈ μƒμ„Έ

### 1. λ¨λΈ λ΅λ“

#### μƒ λ¨λΈ λ΅λ“ (ν•™μµμ©)
```bash
POST /api/chat/qlora/load
```

**μ”μ²­ λ³Έλ¬Έ:**
```json
{
  "model_name": "beomi/Llama-3-Open-Ko-8B",
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"]
}
```

**νλΌλ―Έν„° μ„¤λ…:**
- `model_name`: HuggingFace λ¨λΈ μ΄λ¦„
- `lora_r`: LoRA rank (λ‚®μ„μλ΅ νλΌλ―Έν„° μ μ, κΈ°λ³Έκ°’: 8)
- `lora_alpha`: LoRA scaling factor (κΈ°λ³Έκ°’: 16)
- `lora_dropout`: Dropout λΉ„μ¨ (κΈ°λ³Έκ°’: 0.05)
- `target_modules`: LoRAλ¥Ό μ μ©ν•  λ μ΄μ–΄ (μµμ…)

**μ‘λ‹µ:**
```json
{
  "status": "success",
  "message": "QLoRA λ¨λΈ λ΅λ“ μ™„λ£: beomi/Llama-3-Open-Ko-8B",
  "model_info": {
    "loaded": true,
    "model_name": "beomi/Llama-3-Open-Ko-8B",
    "adapter_path": null,
    "config": {
      "lora_r": 8,
      "lora_alpha": 16,
      "lora_dropout": 0.05
    }
  }
}
```

#### ν•™μµλ λ¨λΈ λ΅λ“
```bash
POST /api/chat/qlora/load_trained?base_model_name=beomi/Llama-3-Open-Ko-8B&adapter_path=./checkpoints/qlora/final_model
```

**μΏΌλ¦¬ νλΌλ―Έν„°:**
- `base_model_name`: λ² μ΄μ¤ λ¨λΈ μ΄λ¦„
- `adapter_path`: ν•™μµλ μ–΄λ‘ν„° κ²½λ΅

### 2. λ¨λΈ μƒνƒ ν™•μΈ

```bash
GET /api/chat/qlora/status
```

**μ‘λ‹µ:**
```json
{
  "loaded": true,
  "model_name": "beomi/Llama-3-Open-Ko-8B",
  "adapter_path": "./checkpoints/qlora/final_model"
}
```

### 3. QLoRA λ¨λΈκ³Ό λ€ν™”

```bash
POST /api/chat/qlora/chat?max_new_tokens=512&temperature=0.7&top_p=0.9
```

**μ”μ²­ λ³Έλ¬Έ:**
```json  
{
  "message": "LangChainμ— λ€ν•΄ μ„¤λ…ν•΄μ¤"
}
```

**μΏΌλ¦¬ νλΌλ―Έν„°:**
- `max_new_tokens`: μƒμ„±ν•  μµλ€ ν† ν° μ (κΈ°λ³Έκ°’: 512)
- `temperature`: μ¨λ„ κ°’ (0.1-1.0, λ†’μ„μλ΅ μ°½μμ , κΈ°λ³Έκ°’: 0.7)
- `top_p`: Top-p μƒν”λ§ (0.1-1.0, κΈ°λ³Έκ°’: 0.9)

**μ‘λ‹µ:**
```json
{
  "answer": "LangChainμ€ λ€ν• μ–Έμ–΄ λ¨λΈμ„ ν™μ©ν• μ• ν”λ¦¬μΌ€μ΄μ… κ°λ°μ„ μ„ν• ν”„λ μ„μ›ν¬μ…λ‹λ‹¤...",
  "sources": ["π¤– μ¶μ²: QLoRA Fine-tuned Model"],
  "timestamp": "2024-12-18T10:30:00",
  "model_info": {
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 512
  }
}
```

### 4. νμΈνλ‹ μ‹¤ν–‰

```bash
POST /api/chat/qlora/train
```

**μ”μ²­ λ³Έλ¬Έ:**
```json
{
  "conversations": [
    {
      "prompt": "LangChainμ΄ λ­μ•Ό?",
      "response": "LangChainμ€ LLM μ• ν”λ¦¬μΌ€μ΄μ… κ°λ°μ„ μ„ν• ν”„λ μ„μ›ν¬μ…λ‹λ‹¤."
    },
    {
      "prompt": "RAGλ” λ¬΄μ—‡μΈκ°€μ”?",
      "response": "RAGλ” κ²€μƒ‰ μ¦κ°• μƒμ„±(Retrieval-Augmented Generation) κΈ°λ²•μ…λ‹λ‹¤."
    }
  ],
  "output_dir": "./checkpoints/qlora",
  "num_train_epochs": 3,
  "per_device_train_batch_size": 4,
  "learning_rate": 0.0002
}
```

**νλΌλ―Έν„° μ„¤λ…:**
- `conversations`: ν•™μµ λ°μ΄ν„° (prompt-response μμ λ°°μ—΄)
- `output_dir`: μ²΄ν¬ν¬μΈνΈ μ €μ¥ κ²½λ΅
- `num_train_epochs`: ν•™μµ μ—ν­ μ
- `per_device_train_batch_size`: λ°°μΉ ν¬κΈ°
- `learning_rate`: ν•™μµλ¥ 

**μ‘λ‹µ:**
```json
{
  "status": "success",
  "message": "QLoRA ν•™μµ μ™„λ£",
  "result": {
    "status": "completed",
    "output_dir": "./checkpoints/qlora",
    "final_model_path": "./checkpoints/qlora/final_model",
    "train_loss": 0.345,
    "epochs": 3,
    "timestamp": "2024-12-18T11:00:00"
  }
}
```

### 5. λ¨λΈ μ–Έλ΅λ“

```bash
POST /api/chat/qlora/unload
```

**μ‘λ‹µ:**
```json
{
  "status": "success",
  "message": "QLoRA λ¨λΈ μ–Έλ΅λ“ μ™„λ£"
}
```

## π“ μ‚¬μ© μ‹λ‚λ¦¬μ¤

### μ‹λ‚λ¦¬μ¤ 1: μƒ λ¨λΈ ν•™μµ λ° μ‚¬μ©

```bash
# Step 1: λ¨λΈ λ΅λ“
curl -X POST "http://localhost:8000/api/chat/qlora/load" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "beomi/Llama-3-Open-Ko-8B",
    "lora_r": 8,
    "lora_alpha": 16
  }'

# Step 2: ν•™μµ λ°μ΄ν„° μ¤€λΉ„ λ° νμΈνλ‹
curl -X POST "http://localhost:8000/api/chat/qlora/train" \
  -H "Content-Type: application/json" \
  -d '{
    "conversations": [
      {"prompt": "μ§λ¬Έ1", "response": "λ‹µλ³€1"},
      {"prompt": "μ§λ¬Έ2", "response": "λ‹µλ³€2"}
    ],
    "num_train_epochs": 3,
    "output_dir": "./checkpoints/my_model"
  }'

# Step 3: ν•™μµλ λ¨λΈκ³Ό λ€ν™”
curl -X POST "http://localhost:8000/api/chat/qlora/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "μ•λ…•ν•μ„Έμ”!"}'

# Step 4: λ©”λ¨λ¦¬ ν•΄μ  (ν•„μ”μ‹)
curl -X POST "http://localhost:8000/api/chat/qlora/unload"
```

### μ‹λ‚λ¦¬μ¤ 2: κΈ°μ΅΄ ν•™μµ λ¨λΈ μ‚¬μ©

```bash
# Step 1: ν•™μµλ μ–΄λ‘ν„° λ΅λ“
curl -X POST "http://localhost:8000/api/chat/qlora/load_trained?base_model_name=beomi/Llama-3-Open-Ko-8B&adapter_path=./checkpoints/my_model/final_model"

# Step 2: μƒνƒ ν™•μΈ
curl -X GET "http://localhost:8000/api/chat/qlora/status"

# Step 3: λ€ν™”ν•κΈ°
curl -X POST "http://localhost:8000/api/chat/qlora/chat?temperature=0.8" \
  -H "Content-Type: application/json" \
  -d '{"message": "LangChainμ— λ€ν•΄ μ„¤λ…ν•΄μ¤"}'
```

## π”§ Python μ½”λ“ μμ 

### ChatService μ§μ ‘ μ‚¬μ©

```python
from app.services.chat_service import ChatService
from app.services.rag_service import RAGService

# μ„λΉ„μ¤ μ΄κΈ°ν™”
rag_service = RAGService(llm, embeddings, repository)
chat_service = ChatService(rag_service)

# 1. QLoRA λ¨λΈ λ΅λ“
model, tokenizer = chat_service.load_qlora_model(
    model_name="beomi/Llama-3-Open-Ko-8B",
    lora_r=8,
    lora_alpha=16
)

# 2. λ€ν™”ν•κΈ°
response = chat_service.chat_with_qlora_model(
    model=model,
    tokenizer=tokenizer,
    message="μ•λ…•ν•μ„Έμ”!",
    temperature=0.7
)
print(response["answer"])

# 3. ν•™μµ λ°μ΄ν„° μ¤€λΉ„
conversations = [
    {"prompt": "μ§λ¬Έ1", "response": "λ‹µλ³€1"},
    {"prompt": "μ§λ¬Έ2", "response": "λ‹µλ³€2"}
]
train_dataset = chat_service.prepare_training_dataset(
    tokenizer=tokenizer,
    conversations=conversations
)

# 4. νμΈνλ‹
result = chat_service.train_qlora_model(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    num_train_epochs=3,
    output_dir="./checkpoints/my_model"
)
print(f"ν•™μµ μ™„λ£! λ¨λΈ μ„μΉ: {result['final_model_path']}")

# 5. ν•™μµλ λ¨λΈ λ΅λ“
trained_model, trained_tokenizer = chat_service.load_trained_qlora_model(
    base_model_name="beomi/Llama-3-Open-Ko-8B",
    adapter_path="./checkpoints/my_model/final_model"
)
```

## β™οΈ μ„¤μ • κ°€μ΄λ“

### LoRA ν•μ΄νΌνλΌλ―Έν„°

#### lora_r (Rank)
- **λ‚®μ€ κ°’ (4-8)**: μ μ€ νλΌλ―Έν„°, λΉ λ¥Έ ν•™μµ, μ‘μ€ λ©”λ¨λ¦¬
- **λ†’μ€ κ°’ (16-32)**: λ§μ€ νλΌλ―Έν„°, λ” λ‚μ€ μ„±λ¥, ν° λ©”λ¨λ¦¬

#### lora_alpha
- μΌλ°μ μΌλ΅ `lora_r`μ 2λ°° μ‚¬μ©
- `lora_r=8`μ΄λ©΄ `lora_alpha=16` κ¶μ¥

#### target_modules
- **Llama κ³„μ—΄**: `["q_proj", "k_proj", "v_proj", "o_proj"]`
- **GPT κ³„μ—΄**: `["c_attn", "c_proj"]`
- **λ¨λ“  Linear λ μ΄μ–΄**: λ” λ§μ€ λ μ΄μ–΄ μ¶”κ°€ κ°€λ¥

### ν•™μµ ν•μ΄νΌνλΌλ―Έν„°

#### num_train_epochs
- **μ‘μ€ λ°μ΄ν„°μ…‹ (< 100)**: 5-10 μ—ν­
- **μ¤‘κ°„ λ°μ΄ν„°μ…‹ (100-1000)**: 3-5 μ—ν­
- **ν° λ°μ΄ν„°μ…‹ (> 1000)**: 1-3 μ—ν­

#### learning_rate
- **κΈ°λ³Έκ°’**: 2e-4 (0.0002)
- **μ‘μ€ λ¨λΈ**: 3e-4
- **ν° λ¨λΈ**: 1e-4

#### per_device_train_batch_size
- GPU λ©”λ¨λ¦¬μ— λ”°λΌ μ΅°μ 
- **8GB GPU**: 1-2
- **16GB GPU**: 2-4
- **24GB GPU**: 4-8

## π― λ² μ¤νΈ ν”„λ™ν‹°μ¤

### 1. λ©”λ¨λ¦¬ κ΄€λ¦¬
```python
# ν•™μµ μ „μ— κΈ°μ΅΄ λ¨λΈ μ–Έλ΅λ“
POST /api/chat/qlora/unload

# μƒ λ¨λΈ λ΅λ“
POST /api/chat/qlora/load
```

### 2. ν•™μµ λ°μ΄ν„° ν’μ§
- μµμ† 50κ° μ΄μƒμ κ³ ν’μ§ λ€ν™” μ μ¤€λΉ„
- μΌκ΄€λ ν•μ‹κ³Ό μ¤νƒ€μΌ μ μ§€
- λ„λ©”μΈ νΉν™” λ°μ΄ν„° μ‚¬μ© κ¶μ¥

### 3. μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬
```bash
# μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ κµ¬μ΅°
checkpoints/
β”β”€β”€ model_v1/
β”‚   β”β”€β”€ checkpoint-100/
β”‚   β”β”€β”€ checkpoint-200/
β”‚   β””β”€β”€ final_model/
β””β”€β”€ model_v2/
    β””β”€β”€ final_model/
```

### 4. λ¨λΈ ν‰κ°€
ν•™μµ ν›„ λ‹¤μ–‘ν• μ§λ¬ΈμΌλ΅ ν…μ¤νΈ:
```bash
# ν…μ¤νΈ μ¤ν¬λ¦½νΈ
for question in "${questions[@]}"; do
  curl -X POST "http://localhost:8000/api/chat/qlora/chat" \
    -H "Content-Type: application/json" \
    -d "{\"message\": \"$question\"}"
done
```

## β οΈ μ£Όμμ‚¬ν•­

### 1. GPU λ©”λ¨λ¦¬
- μµμ† 8GB GPU κ¶μ¥ (4bit μ–‘μν™” μ‚¬μ© μ‹)
- λ¨λΈ ν¬κΈ°μ— λ”°λΌ μ”κµ¬ μ‚¬ν•­ λ‹¤λ¦„:
  - 7B λ¨λΈ: 8GB GPU
  - 13B λ¨λΈ: 16GB GPU
  - 70B λ¨λΈ: 24GB+ GPU

### 2. ν•™μµ μ‹κ°„
- λ°μ΄ν„°μ…‹ ν¬κΈ°μ™€ μ—ν­ μμ— λΉ„λ΅€
- μμƒ μ‹κ°„:
  - 100 μƒν”, 3 μ—ν­: 10-20λ¶„
  - 1000 μƒν”, 3 μ—ν­: 1-2μ‹κ°„

### 3. μ¤λ²„ν”Όν… λ°©μ§€
- λ„λ¬΄ λ§μ€ μ—ν­ ν”Όν•κΈ°
- λ‹¤μ–‘ν• λ°μ΄ν„°λ΅ ν•™μµ
- Validation setμΌλ΅ λ¨λ‹ν„°λ§

### 4. API νƒ€μ„μ•„μ›ƒ
ν•™μµ μ¤‘μ—λ” μ‘λ‹µμ΄ μ¤λ κ±Έλ¦΄ μ μμΌλ―€λ΅ ν΄λΌμ΄μ–ΈνΈ νƒ€μ„μ•„μ›ƒ μ„¤μ • μ¦κ°€:
```python
import httpx

async with httpx.AsyncClient(timeout=3600.0) as client:
    response = await client.post("http://localhost:8000/api/chat/qlora/train", ...)
```

## π” νΈλ¬λΈ”μν…

### λ¬Έμ : Out of Memory
**ν•΄κ²°:**
- `per_device_train_batch_size` μ¤„μ΄κΈ°
- `gradient_accumulation_steps` λλ¦¬κΈ°
- λ” μ‘μ€ `lora_r` μ‚¬μ©

### λ¬Έμ : ν•™μµ μ†λ„κ°€ λ„λ¬΄ λλ¦Ό
**ν•΄κ²°:**
- `gradient_accumulation_steps` μ¤„μ΄κΈ°
- `per_device_train_batch_size` λλ¦¬κΈ° (λ©”λ¨λ¦¬ ν—μ© μ‹)
- λ” κ°•λ ¥ν• GPU μ‚¬μ©

### λ¬Έμ : λ¨λΈ λ΅λ“ μ‹¤ν¨
**ν•΄κ²°:**
- λ¨λΈ μ΄λ¦„ ν™•μΈ
- HuggingFace ν† ν° μ„¤μ • (λΉ„κ³µκ° λ¨λΈμΈ κ²½μ°)
- λ””μ¤ν¬ κ³µκ°„ ν™•μΈ

## π“ μ°Έκ³  μλ£

- [PEFT Documentation](https://huggingface.co/docs/peft)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Llama-3-Open-Ko-8B](https://huggingface.co/beomi/Llama-3-Open-Ko-8B)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)

## π“ λ‹¤μ λ‹¨κ³„

1. **κΈ°μ΅΄ RAG μ‹μ¤ν…κ³Ό ν†µν•©**: QLoRA λ¨λΈμ— RAG κ²€μƒ‰ κ²°κ³Όλ¥Ό μ»¨ν…μ¤νΈλ΅ μ κ³µ
2. **λ¨λΈ μ•™μƒλΈ”**: μ—¬λ¬ νμΈνλ‹ λ¨λΈ κ²°κ³Ό κ²°ν•©
3. **μλ™ ν‰κ°€**: ν•™μµ ν›„ μλ™ ν’μ§ ν‰κ°€ νμ΄ν”„λΌμΈ κµ¬μ¶•
4. **λ°°ν¬ μµμ ν™”**: ν”„λ΅λ•μ… ν™κ²½μ„ μ„ν• μ¶”λ΅  μµμ ν™”

---

**λ§μ§€λ§‰ μ—…λ°μ΄νΈ:** 2024-12-18
**μ‘μ„±μ:** AI Assistant
**λ²„μ „:** 1.0

