# ğŸ› ï¸ ë¡œì»¬ Llama ëª¨ë¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [ë°©ë²• 1: HuggingFace Transformers ì‚¬ìš©](#ë°©ë²•-1-huggingface-transformers-ì‚¬ìš©)
3. [ë°©ë²• 2: llama.cpp ì‚¬ìš© (ê¶Œì¥)](#ë°©ë²•-2-llamacpp-ì‚¬ìš©-ê¶Œì¥)
4. [ì˜ì¡´ì„± ì£¼ì…](#ì˜ì¡´ì„±-ì£¼ì…)
5. [í™˜ê²½ ë³€ìˆ˜ ì„¤ì •](#í™˜ê²½-ë³€ìˆ˜-ì„¤ì •)

---

## í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# HuggingFace Transformers ì‚¬ìš© ì‹œ
pip install transformers torch langchain-huggingface accelerate

# llama.cpp ì‚¬ìš© ì‹œ (ë” ë¹ ë¦„, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
pip install llama-cpp-python langchain-community
```

---

## ë°©ë²• 1: HuggingFace Transformers ì‚¬ìš©

### Step 1: `app/api/dependencies.py` ìˆ˜ì •

```python
from functools import lru_cache
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline

from app.models.providers.custom_provider import CustomLLM
from app.models.base import BaseLLM

@lru_cache()
def get_llm() -> BaseLLM:
    """ë¡œì»¬ Llama ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""

    # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_path = "app/models/midm"

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",  # GPU ìë™ í• ë‹¹
        torch_dtype="auto",  # ìë™ íƒ€ì… ì„ íƒ
        low_cpu_mem_usage=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # 2. Pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
    )

    # 3. LangChain ë˜í¼ë¡œ ë³€í™˜
    hf_pipeline = HuggingFacePipeline(pipeline=pipe)

    # 4. CustomLLMìœ¼ë¡œ ë˜í•‘í•˜ì—¬ ë°˜í™˜
    return CustomLLM(
        model=hf_pipeline,
        model_name="local-llama-1.2b"
    )
```

### Step 2: ì„œë²„ ì‹¤í–‰

```bash
uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000 --reload
```

---

## ë°©ë²• 2: llama.cpp ì‚¬ìš© (ê¶Œì¥)

### Step 1: ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•„ìš” ì‹œ)

```bash
# safetensors â†’ GGUF ë³€í™˜
pip install llama-cpp-python

# ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ (HuggingFaceì—ì„œ ì œê³µ)
python convert-hf-to-gguf.py --outfile model.gguf app/models/midm/
```

### Step 2: `app/api/dependencies.py` ìˆ˜ì •

```python
from functools import lru_cache
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler

from app.models.providers.custom_provider import CustomLLM
from app.models.base import BaseLLM

@lru_cache()
def get_llm() -> BaseLLM:
    """ë¡œì»¬ Llama ëª¨ë¸ì„ llama.cppë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""

    # Callback ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

    # llama.cppë¡œ ëª¨ë¸ ë¡œë“œ
    llm = LlamaCpp(
        model_path="app/models/midm/model.gguf",  # GGUF í˜•ì‹
        temperature=0.7,
        max_tokens=512,
        n_ctx=2048,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        callback_manager=callback_manager,
        verbose=False,
        n_gpu_layers=0,  # CPU ì‚¬ìš© (GPU ì‚¬ìš© ì‹œ ê°’ ì¦ê°€)
    )

    return CustomLLM(
        model=llm,
        model_name="local-llama-cpp"
    )
```

---

## ì˜ì¡´ì„± ì£¼ì…

### í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì „í™˜

```python
# app/api/dependencies.py
import os
from functools import lru_cache

@lru_cache()
def get_llm() -> BaseLLM:
    provider = os.getenv("LLM_PROVIDER", "openai")

    if provider == "local_llama":
        # ë¡œì»¬ Llama ëª¨ë¸ ë¡œë“œ
        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
        from langchain_huggingface import HuggingFacePipeline

        model_path = os.getenv("LOCAL_MODEL_PATH", "app/models/midm")

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype="auto",
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
        )

        hf_pipeline = HuggingFacePipeline(pipeline=pipe)

        return CustomLLM(model=hf_pipeline, model_name="local-llama")

    elif provider == "openai":
        # OpenAI ëª¨ë¸ ì‚¬ìš©
        from app.models.factory import ModelFactory
        return ModelFactory.create_llm()

    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
```

---

## í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### `.env` íŒŒì¼

```bash
# OpenAI ì‚¬ìš© (ê¸°ë³¸)
LLM_PROVIDER=openai
OPENAI_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_api_key_here

# ë¡œì»¬ Llama ëª¨ë¸ ì‚¬ìš©
LLM_PROVIDER=local_llama
LOCAL_MODEL_PATH=app/models/midm
LOCAL_MODEL_DEVICE=cpu  # ë˜ëŠ” cuda

# Embeddings (OpenAI ì‚¬ìš©)
EMBEDDINGS_PROVIDER=openai
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small
```

---

## ğŸš€ ì‹¤í–‰ ì˜ˆì‹œ

### OpenAI ì‚¬ìš©

```bash
export LLM_PROVIDER=openai
export OPENAI_API_KEY=sk-...

uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000
```

### ë¡œì»¬ Llama ëª¨ë¸ ì‚¬ìš©

```bash
export LLM_PROVIDER=local_llama
export LOCAL_MODEL_PATH=app/models/midm

uvicorn app.api_server_refactored:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“ í…ŒìŠ¤íŠ¸

```python
# test_local_model.py
import requests

response = requests.post(
    "http://localhost:8000/api/chat/general",
    json={"message": "ì•ˆë…•í•˜ì„¸ìš”!"}
)

print(response.json())
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬**: Llama ëª¨ë¸ì€ ìµœì†Œ 8GB RAM í•„ìš” (1.2B ëª¨ë¸ ê¸°ì¤€)
2. **ì†ë„**: CPU ì‚¬ìš© ì‹œ ëŠë¦´ ìˆ˜ ìˆìŒ. GPU ê¶Œì¥
3. **GGUF ë³€í™˜**: llama.cpp ì‚¬ìš© ì‹œ ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•¨
4. **ì˜ì¡´ì„±**: `transformers`, `torch`, `langchain-huggingface` ì„¤ì¹˜ í•„ìš”

---

## ğŸ¯ ê¶Œì¥ ì‚¬í•­

- **ê°œë°œ/í…ŒìŠ¤íŠ¸**: OpenAI ì‚¬ìš© (ë¹ ë¥´ê³  ì•ˆì •ì )
- **í”„ë¡œë•ì…˜ (ë¹„ìš© ì ˆê°)**: ë¡œì»¬ Llama ëª¨ë¸ + llama.cpp
- **í”„ë¡œë•ì…˜ (ì„±ëŠ¥ ì¤‘ìš”)**: OpenAI ë˜ëŠ” GPU ì„œë²„ì— ë¡œì»¬ ëª¨ë¸

